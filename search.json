[{"title":"test","url":"/%E5%AD%A6%E4%B9%A0/%E5%BB%BA%E7%AB%99/test/","content":"hello, world!\n\n建站记录\nLaTeX\\LaTeXLATE​X\n","categories":["学习","建站"]},{"title":"建站记录","url":"/%E5%AD%A6%E4%B9%A0/%E5%BB%BA%E7%AB%99/%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95/","content":"参考:\nObsidian + Hexo + Github Pages建站\nHexo\nObsidian+Git完美维护Hexo博客\n支持渲染双向链接:\nHexo 博客适配 Obsidian 新语法\n图床配置\n腾讯云对象存储COS自建图床并配置Obsidian自动上传\nNext主题配置\nNext\n","categories":["学习","建站"],"tags":["Obsidian","Hexo"]},{"title":"2025-09-14-周日-一周总结","url":"/%E7%94%9F%E6%B4%BB/%E5%91%A8%E8%AE%B0/2025-09-14-%E5%91%A8%E6%97%A5-%E4%B8%80%E5%91%A8%E6%80%BB%E7%BB%93/","content":"过去一周总结\n也是在摆了一个暑假后重新开始写周记了，既然连奶龙同学现在都能坚持日更那我肯定是要坚持周更了。\n开学第一周感觉大多数课也都没上啥实质性的内容，然后在经历了暑假编程集训和数模国赛的拷打以后这一周也基本上是摆过去了，而且在周五开始感冒，更是天崩开局。\n不过还是可以搞一个锐评大二上课程第一印象从夯到拉的:\n\nAI数学基础：课没什么问题，老师也没什么问题，可能最大的问题是我数学太拉，综合给到人上人。\n数据结构：夯。窦老师很能活跃班级气氛而且讲的非常深入浅出，给的思考题也很有趣，真神。\n区块链：个性化选修课，目前来看可能也比较偏向通识课，不会讲一些太深的东西，内容本身还是比较有趣的，第一节课甚至花了小半节课来讲中本聪野史（）。综合给个人上人。\nAI引论：课的内容看上去覆盖面很广也比较有趣，第一周作业居然是给一个小故事生成AI视频，老师也很有亲和力，综合可以给到顶级。\n概统：前两节课讲的基本都是高中时期的内容，而且正好卡在下午最容易昏睡的时间点，不过看前人的评价好像还不错，我现在还不好评价，先给到一个人上人。\nICS：本来想给到夯的，因为助教活人感确实强而且准备的资料（包括作业之类的）确实能感觉出来很用心，甚至还专门搞了个答疑论坛用来课程答疑。结果什么叫第一周因为讲前几章的老师有事所以我们直接从第六章开始讲？那岂不是意味着国庆就有很大的可能要直接开做cache lab？那种事情不要啊。而且这老师讲话语速快的就离谱，感觉像词烫嘴。那最后只能先给到顶级，看看后面情况了。\n英语演讲：老师一眼好，事一眼多，对冲一下给到人上人。\n游泳：夯。首先是学校游泳馆设施就很不错而且水质也好，远超于我暑假里去游的露天游泳池，至少里面不会随机刷新树叶或者虫子；其次是老师也很专业很有趣；最后是不仅可以提前一点跑路还能顺带解决当天的洗澡问题。唯一的问题是人比较多游起来要小心点。\n马原、膝盖：没啥说的，这俩一个拉一个入机。唯一的区别在于膝盖第一周没布置作业而马原居然已经布置了一篇小论文，我真没招了。\n\n等到大二上结束的时候再回来看看，希望没有什么回旋镖。\n\n未来一周展望\n下周就可以步入正轨了，大二上一定好好努力！\n首先得想办法给小论文水掉😅难绷了。\n然后备战一下9月21号的CSP吧，这次争取再进步个十几二十分。\n\n","categories":["生活","周记"]},{"title":"解决AttributeError Can't get attribute on module","url":"/%E6%8A%80%E6%9C%AF/%E6%9D%82%E9%A1%B9/%E8%A7%A3%E5%86%B3AttributeError%20Can't%20get%20attribute%20on%20module/","content":"之前本地跑大作业时遇上的神秘问题，有点抽象，遂记录一下。\n问题\n本地跑大作业的模型训练时一直卡着不动，看jupyter命令行发现报错:AttributeError: Can't get attribute 'FashionDataset' on &lt;module '__main__' (&lt;class '_frozen_importlib.BuiltinImporter'&gt;)&gt;\n在网上搜索一番后在这篇文章发现了问题原因:\n\n实际上这个地方：在linux中使用的是fork的方式，可以复制公共变量和方法到新开启的进程中，但是windows中是重建一个新的进程(全新)，在进程中并不存在公共的变量和方法，所以会出现这个问题，未能导入模块中定义的对象\n\n解决\n把出问题的代码(我这里是FashionDataset类)打包成一个单独的.py文件再import进来就行。\n# dataset.pyimport osimport gzipimport numpy as npimport torchfrom torch.utils.data import Datasetclass FashionDataset(Dataset):    def __init__(self, datadir, transform, is_train=True):        super().__init__()        self.datadir = datadir        self.transform = transform        self.img, self.label = self.load_data(datadir, is_train)        self.len_data = len(self.img)    def __getitem__(self, index):        return self.transform(self.img[index]), self.label[index]    def __len__(self):        return self.len_data    def load_data(self, datadir, is_train):        dirname = os.path.join(datadir)        files = [&#x27;train-labels-idx1-ubyte.gz&#x27;, &#x27;train-images-idx3-ubyte.gz&#x27;,                 &#x27;t10k-labels-idx1-ubyte.gz&#x27;, &#x27;t10k-images-idx3-ubyte.gz&#x27;]        paths = [os.path.join(dirname, f) for f in files]        if is_train:            with gzip.open(paths[0], &#x27;rb&#x27;) as lbpath:                label = np.frombuffer(lbpath.read(), np.uint8, offset=8)            with gzip.open(paths[1], &#x27;rb&#x27;) as imgpath:                img = np.frombuffer(imgpath.read(), np.uint8,                                    offset=16).reshape(len(label), 28, 28)        else:            with gzip.open(paths[2], &#x27;rb&#x27;) as lbpath:                label = np.frombuffer(lbpath.read(), np.uint8, offset=8)            with gzip.open(paths[3], &#x27;rb&#x27;) as imgpath:                img = np.frombuffer(imgpath.read(), np.uint8,                                    offset=16).reshape(len(label), 28, 28)        return img, label\n然后在主文件中from dataset import FashionDataset就行\n","categories":["技术","杂项"],"tags":["bug","Python"]},{"title":"ML2022-Lecture1-Notes","url":"/%E5%AD%A6%E4%B9%A0/NTU-ML-2022/ML2022-Lecture1-Notes/","content":"课程网站\nWhat is Machine Learning?\n\nMachine Learning ≈ Looking for Function\n\nDifferent types\n\nRegression(回归): 输出要预测的数值\nClassification(分类): 输出给定的options中的某一个\nStructured Learing: 创造有结构的物件(作图,写文章)\n\n从最基本的Linear Model开始\n例: 预测youtube频道第二天播放量\n\n构建模型\n基本术语:\n\nmodel: 带有未知参数的函数, 这里就是y=b+wx1y = b + wx_1y=b+wx1​\nfeature: x1x_1x1​, 已知的数据\nweight, bias: 分别是www和bbb\n\n定义损失函数\n一般可以表示为:\nL=1N∑nenL = \\frac{1}{N}\\sum_{n}e_n\nL=N1​n∑​en​\n其中e代表每组数据预测值跟真实值的误差, e可以有不同的计算方法, 如MAE/MSE等。\nError Surface\n测试不同的参数后画出的Loss的等高线图\n\n最优化(optimization)\n梯度下降(gradient descent)\n\n随机挑选初始值w0,b0w^0,b^0w0,b0\n更新参数:\n\nw1←w0−η∂L∂w∣w=w0,b=b0b1←b0−η∂L∂b∣w=w0,b=b0\\begin{align*} w^1 &amp;\\leftarrow w^0 - \\eta \\left. \\frac{\\partial L}{\\partial w} \\right|_{w=w^0, b=b^0} \\\\ b^1 &amp;\\leftarrow b^0 - \\eta \\left. \\frac{\\partial L}{\\partial b} \\right|_{w=w^0, b=b^0} \\end{align*}\nw1b1​←w0−η∂w∂L​​w=w0,b=b0​←b0−η∂b∂L​​w=w0,b=b0​​\n\n迭代\n\n其中η\\etaη就是学习率(learning rate), 一般要手动设定。\n我们把ML中需要手动设定的参数称为hyperparameter\n问题: 局部最优解(local minima) (?)\n\n并非真正的难题\n更加复杂的model\n关于激活函数\n\n大部分复杂的函数图像理论上都可以使用大量的蓝色折线图叠加来表达\n但是我们又该怎么表示这个「蓝色的折线图」呢?\n🤓☝️我们有sigmoid函数\ny=c11+e−(b+wx1)=c sigmoid(b+wx1)\\begin{align*} y &amp;= c \\frac{1}{1 + e^{-(b + wx_1)}} \\\\ &amp;= c\\,sigmoid(b + wx_1) \\end{align*}\ny​=c1+e−(b+wx1​)1​=csigmoid(b+wx1​)​\n而实际上这条「蓝色的折线图」就通常被称为「Hard Sigmoid」\n改变参数对图像的影响:\n\n除了sigmoid函数, 我们还可以用ReLU函数:\n\n那么对于这样的sigmoid函数或者ReLU函数或者其他的函数, 我们就将其称为激活函数。\n以下我们基于使用sigmoid函数的情况。\nNeural Network\n对于一组feature, 我们有:\ny=b+∑ici sigmoid(bi+wix1)y = b + \\sum_{i}c_i\\,sigmoid(b_i + w_ix_1)\ny=b+i∑​ci​sigmoid(bi​+wi​x1​)\n如果我们想要同时考虑 jjj 组feature:\ny=b+∑ici sigmoid(bi+∑jwijxj)y = b + \\sum_{i}c_i\\,sigmoid(b_i + \\sum_{j}w_{ij}x_{j})\ny=b+i∑​ci​sigmoid(bi​+j∑​wij​xj​)\n我们引入线性代数中向量与矩阵的概念来简便地表示y:\n\n然后我们把a1,a2,...a_1,a_2,...a1​,a2​,...继续作为新的xxx, 引入更多未知的参数来让函数更复杂:\n\n然后我们就有了Neural Network (:\n\n\n为什么不在一层上增加更多的节点让网络「变宽」而是要多做几层让网络「变深」?\n以后再讲(((\n(后续可能会补链接)\n\n\n为什么不能一直增加深度?\n容易带来过拟合的问题: 在训练集上表现变好但在未知数据上表现变差\n\n\n","categories":["学习","NTU-ML-2022"],"tags":["Notes","AI"]}]