[{"title":"解决AttributeError Can't get attribute on module","url":"/%E6%8A%80%E6%9C%AF/%E6%9D%82%E9%A1%B9/%E8%A7%A3%E5%86%B3AttributeError%20Can't%20get%20attribute%20on%20module/","content":"之前本地跑大作业时遇上的神秘问题，有点抽象，遂记录一下。\n问题\n本地跑大作业的模型训练时一直卡着不动，看jupyter命令行发现报错:AttributeError: Can't get attribute 'FashionDataset' on &lt;module '__main__' (&lt;class '_frozen_importlib.BuiltinImporter'&gt;)&gt;\n在网上搜索一番后在这篇文章发现了问题原因:\n\n实际上这个地方：在linux中使用的是fork的方式，可以复制公共变量和方法到新开启的进程中，但是windows中是重建一个新的进程(全新)，在进程中并不存在公共的变量和方法，所以会出现这个问题，未能导入模块中定义的对象\n\n解决\n把出问题的代码(我这里是FashionDataset类)打包成一个单独的.py文件再import进来就行。\n# dataset.pyimport osimport gzipimport numpy as npimport torchfrom torch.utils.data import Datasetclass FashionDataset(Dataset):    def __init__(self, datadir, transform, is_train=True):        super().__init__()        self.datadir = datadir        self.transform = transform        self.img, self.label = self.load_data(datadir, is_train)        self.len_data = len(self.img)    def __getitem__(self, index):        return self.transform(self.img[index]), self.label[index]    def __len__(self):        return self.len_data    def load_data(self, datadir, is_train):        dirname = os.path.join(datadir)        files = [&#x27;train-labels-idx1-ubyte.gz&#x27;, &#x27;train-images-idx3-ubyte.gz&#x27;,                 &#x27;t10k-labels-idx1-ubyte.gz&#x27;, &#x27;t10k-images-idx3-ubyte.gz&#x27;]        paths = [os.path.join(dirname, f) for f in files]        if is_train:            with gzip.open(paths[0], &#x27;rb&#x27;) as lbpath:                label = np.frombuffer(lbpath.read(), np.uint8, offset=8)            with gzip.open(paths[1], &#x27;rb&#x27;) as imgpath:                img = np.frombuffer(imgpath.read(), np.uint8,                                    offset=16).reshape(len(label), 28, 28)        else:            with gzip.open(paths[2], &#x27;rb&#x27;) as lbpath:                label = np.frombuffer(lbpath.read(), np.uint8, offset=8)            with gzip.open(paths[3], &#x27;rb&#x27;) as imgpath:                img = np.frombuffer(imgpath.read(), np.uint8,                                    offset=16).reshape(len(label), 28, 28)        return img, label\n然后在主文件中from dataset import FashionDataset就行\n","categories":["技术","杂项"],"tags":["bug","Python"]},{"title":"2025-09-14-周日-一周总结","url":"/%E7%94%9F%E6%B4%BB/%E5%91%A8%E8%AE%B0/2025-09-14-%E5%91%A8%E6%97%A5-%E4%B8%80%E5%91%A8%E6%80%BB%E7%BB%93/","content":"过去一周总结\n也是在摆了一个暑假后重新开始写周记了，既然连奶龙同学现在都能坚持日更那我肯定是要坚持周更了。\n开学第一周感觉大多数课也都没上啥实质性的内容，然后在经历了暑假编程集训和数模国赛的拷打以后这一周也基本上是摆过去了，而且在周五开始感冒，更是天崩开局。\n不过还是可以搞一个锐评大二上课程第一印象从夯到拉的:\n\nAI数学基础：课没什么问题，老师也没什么问题，可能最大的问题是我数学太拉，综合给到人上人。\n数据结构：夯。窦老师很能活跃班级气氛而且讲的非常深入浅出，给的思考题也很有趣，真神。\n区块链：个性化选修课，目前来看可能也比较偏向通识课，不会讲一些太深的东西，内容本身还是比较有趣的，第一节课甚至花了小半节课来讲中本聪野史（）。综合给个人上人。\nAI引论：课的内容看上去覆盖面很广也比较有趣，第一周作业居然是给一个小故事生成AI视频，老师也很有亲和力，综合可以给到顶级。\n概统：前两节课讲的基本都是高中时期的内容，而且正好卡在下午最容易昏睡的时间点，不过看前人的评价好像还不错，我现在还不好评价，先给到一个人上人。\nICS：本来想给到夯的，因为助教活人感确实强而且准备的资料（包括作业之类的）确实能感觉出来很用心，甚至还专门搞了个答疑论坛用来课程答疑。结果什么叫第一周因为讲前几章的老师有事所以我们直接从第六章开始讲？那岂不是意味着国庆就有很大的可能要直接开做cache lab？那种事情不要啊。而且这老师讲话语速快的就离谱，感觉像词烫嘴。那最后只能先给到顶级，看看后面情况了。\n英语演讲：老师一眼好，事一眼多，对冲一下给到人上人。\n游泳：夯。首先是学校游泳馆设施就很不错而且水质也好，远超于我暑假里去游的露天游泳池，至少里面不会随机刷新树叶或者虫子；其次是老师也很专业很有趣；最后是不仅可以提前一点跑路还能顺带解决当天的洗澡问题。唯一的问题是人比较多游起来要小心点。\n马原、膝盖：没啥说的，这俩一个拉一个入机。唯一的区别在于膝盖第一周没布置作业而马原居然已经布置了一篇小论文，我真没招了。\n\n等到大二上结束的时候再回来看看，希望没有什么回旋镖。\n\n未来一周展望\n下周就可以步入正轨了，大二上一定好好努力！\n首先得想办法给小论文水掉😅难绷了。\n然后备战一下9月21号的CSP吧，这次争取再进步个十几二十分。\n\n","categories":["生活","周记"]},{"title":"2025-09-21-周日-一周总结","url":"/%E7%94%9F%E6%B4%BB/%E5%91%A8%E8%AE%B0/2025-09-21-%E5%91%A8%E6%97%A5-%E4%B8%80%E5%91%A8%E6%80%BB%E7%BB%93/","content":"过去一周总结\nCSP大败而归了，12月再战。\n最近的身体状况不是很好，这周头上感冒刚好，之前拔智齿的拔牙窝附近又开始痛，似乎是牙槽骨凸出来了一块，给牙龈和旁边的舌头顶坏了，参看这个知乎问题，不过到目前写文章的时候已经不怎么痛了。\n总之这一周的情况就是感觉很累但也不知道累什么，跟身体的情况可能也有一点关系，不过好歹学校的作业是完成的差不多了，除了下周英语演讲课要做的自我介绍还没有什么头绪。\n周六一天说是准备CSP实际也不知道要准备啥，毕竟算法基础太薄弱了临时抱佛脚用处也有限，遂打游戏，给鸣潮一堆活动做掉了然后抽了个夏空来补完整卡提配队。\n考完CSP出来，看到优秀学长（存疑）牢记发表的文章入土了才是上岸了，顿时又释然了。在海里扑腾的人不一定要上岸，不淹死就行了。\n提到了牢记就能顺便提一下目前关注的优质个人自媒体博主：躺刑记诉四大金刚（躺平学原理、刑法学爱好者（公众号已改名TENET.INKER）、记之而去、此处不宜诉讼），构成了我对当下时政热点的大部分信息和观点来源。有时候也挺羡慕这些学文科的写文章表达观点就像呼吸一样自然，而我每周的周记都得憋一会，还憋不出来多少个字。\n\n未来一周展望\n下周数据结构和AI引论都要开始上机了，周中应该会变得更忙。更难绷的是周末又要调休还调的是周二的课，这天除了没早八基本上要一直上到晚上。而这也就意味着这个九月一共四个周末：第一个周末在被国赛拷打；第二个周末在被感冒拷打；这个周末被CSP拷打；下个周末被调休拷打。这是在干什么，就不能让我有一个正常的周末吗（恼）。\n总而言之首要目标是活到国庆，顺便打下周的崩铁更新以及084×粥联动。\n\n","categories":["生活","周记"]},{"title":"2025-09-28-周日-一周总结","url":"/%E7%94%9F%E6%B4%BB/%E5%91%A8%E8%AE%B0/2025-09-28-%E5%91%A8%E6%97%A5-%E4%B8%80%E5%91%A8%E6%80%BB%E7%BB%93/","content":"过去一周总结\n首先严格来说写这篇周记的时间已经是周一了，这是因为调休导致忘记了昨天是周日应当写周记，不过刚刚又想起来了，趁为时未晚赶紧补上。\n上周事情还是非常多的，首先是准备英语演讲的自我介绍，我从高中时期每周挤一个多小时地铁上下学的角度切入讲了一个非常🍬的自嘲式自我介绍，令人忍俊不禁。奶龙同学曾经认为像我高中时期这样不看手机不睡觉挤一个半小时地铁回家会使他变成精神病，侧面证明了这篇自我介绍应该还能算勉强达成「具有新意」的要求。\n然后从周三开始就开始猛攻cache lab，先花了一点时间拿下了partA的cache模拟，接着花了大量的时间在思考partB的优化上，截至目前为止还能暂时性地占据排行榜榜一，不过现在case2和case3的调优都碰上瓶颈了，也不知道还能榜一多久。\n目前的三个大作业：\n数据结构：实现顺序表和单链表两个数据结构，然后基于其中之一搓一堆字符串处理相关的功能。这个基本还没怎么动。\nAI引论：以代码填空的形式完成一个简单的井字棋小游戏，重点是通过alpha-beta剪枝来优化电脑的搜索方式。这个比较简单，很快就做完了。令人难绷的是给的代码模板有一个小bug导致无法选择谁先手，其实是对or运算符的不规范使用导致的。\nICS：没啥要说的了，暂时不会再花更多的时间想case2和case3的更优解了。\n实际上大作业的DDL都在国庆后，而现在已经基本上算是解决了两个，可喜可贺。\n还有你怎么知道我崩铁一次不歪拿下11长夜月，以及084两百抽出二井一成功集齐联动EGO🥳\n\n未来一周展望\n先想办法给最麻烦的数据结构大作业写了，然后等国庆回去再把英语演讲要求拍的视频拍了，剩下的时间就都可以用来干别的事了。至于具体要干什么还没怎么想好，等放假了慢慢想。\n\n"},{"title":"ML2022-Lecture1-Notes","url":"/%E5%AD%A6%E4%B9%A0/NTU-ML-2022/ML2022-Lecture1-Notes/","content":"课程网站\nWhat is Machine Learning?\n\nMachine Learning ≈ Looking for Function\n\nDifferent types\n\nRegression(回归): 输出要预测的数值\nClassification(分类): 输出给定的options中的某一个\nStructured Learing: 创造有结构的物件(作图,写文章)\n\n从最基本的Linear Model开始\n例: 预测youtube频道第二天播放量\n\n构建模型\n基本术语:\n\nmodel: 带有未知参数的函数, 这里就是y=b+wx1y = b + wx_1y=b+wx1​\nfeature: x1x_1x1​, 已知的数据\nweight, bias: 分别是www和bbb\n\n定义损失函数\n一般可以表示为:\nL=1N∑nenL = \\frac{1}{N}\\sum_{n}e_n\nL=N1​n∑​en​\n其中e代表每组数据预测值跟真实值的误差, e可以有不同的计算方法, 如MAE/MSE等。\nError Surface\n测试不同的参数后画出的Loss的等高线图\n\n最优化(optimization)\n梯度下降(gradient descent)\n\n随机挑选初始值w0,b0w^0,b^0w0,b0\n更新参数:\n\nw1←w0−η∂L∂w∣w=w0,b=b0b1←b0−η∂L∂b∣w=w0,b=b0\\begin{align*} w^1 &amp;\\leftarrow w^0 - \\eta \\left. \\frac{\\partial L}{\\partial w} \\right|_{w=w^0, b=b^0} \\\\ b^1 &amp;\\leftarrow b^0 - \\eta \\left. \\frac{\\partial L}{\\partial b} \\right|_{w=w^0, b=b^0} \\end{align*}\nw1b1​←w0−η∂w∂L​​w=w0,b=b0​←b0−η∂b∂L​​w=w0,b=b0​​\n\n迭代\n\n其中η\\etaη就是学习率(learning rate), 一般要手动设定。\n我们把ML中需要手动设定的参数称为hyperparameter\n问题: 局部最优解(local minima) (?)\n\n并非真正的难题\n更加复杂的model\n关于激活函数\n\n大部分复杂的函数图像理论上都可以使用大量的蓝色折线图叠加来表达\n但是我们又该怎么表示这个「蓝色的折线图」呢?\n🤓☝️我们有sigmoid函数\ny=c11+e−(b+wx1)=c sigmoid(b+wx1)\\begin{align*} y &amp;= c \\frac{1}{1 + e^{-(b + wx_1)}} \\\\ &amp;= c\\,sigmoid(b + wx_1) \\end{align*}\ny​=c1+e−(b+wx1​)1​=csigmoid(b+wx1​)​\n而实际上这条「蓝色的折线图」就通常被称为「Hard Sigmoid」\n改变参数对图像的影响:\n\n除了sigmoid函数, 我们还可以用ReLU函数:\n\n那么对于这样的sigmoid函数或者ReLU函数或者其他的函数, 我们就将其称为激活函数。\n以下我们基于使用sigmoid函数的情况。\nNeural Network\n对于一组feature, 我们有:\ny=b+∑ici sigmoid(bi+wix1)y = b + \\sum_{i}c_i\\,sigmoid(b_i + w_ix_1)\ny=b+i∑​ci​sigmoid(bi​+wi​x1​)\n如果我们想要同时考虑 jjj 组feature:\ny=b+∑ici sigmoid(bi+∑jwijxj)y = b + \\sum_{i}c_i\\,sigmoid(b_i + \\sum_{j}w_{ij}x_{j})\ny=b+i∑​ci​sigmoid(bi​+j∑​wij​xj​)\n我们引入线性代数中向量与矩阵的概念来简便地表示y:\n\n然后我们把a1,a2,...a_1,a_2,...a1​,a2​,...继续作为新的xxx, 引入更多未知的参数来让函数更复杂:\n\n然后我们就有了Neural Network (:\n\n\n为什么不在一层上增加更多的节点让网络「变宽」而是要多做几层让网络「变深」?\n以后再讲(((\n(后续可能会补链接)\n\n\n为什么不能一直增加深度?\n容易带来过拟合的问题: 在训练集上表现变好但在未知数据上表现变差\n\n\n","categories":["学习","NTU-ML-2022"],"tags":["Notes","AI"]},{"title":"建站记录","url":"/%E5%AD%A6%E4%B9%A0/%E5%BB%BA%E7%AB%99/%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95/","content":"参考:\nObsidian + Hexo + Github Pages建站\nHexo\nObsidian+Git完美维护Hexo博客\n支持渲染双向链接:\nHexo 博客适配 Obsidian 新语法\n图床配置\n腾讯云对象存储COS自建图床并配置Obsidian自动上传\nNext主题配置\nNext\n","categories":["学习","建站"],"tags":["Obsidian","Hexo"]},{"title":"test","url":"/%E5%AD%A6%E4%B9%A0/%E5%BB%BA%E7%AB%99/test/","content":"hello, world!\n\n建站记录\nLaTeX\\LaTeXLATE​X\n","categories":["学习","建站"]}]