[{"title":"test","url":"/%E5%AD%A6%E4%B9%A0/%E5%BB%BA%E7%AB%99/test/","content":"hello, world!\n\n建站记录\nLaTeX\\LaTeXLATE​X\n","categories":["学习","建站"]},{"title":"ML2022-Lecture1-Notes","url":"/%E5%AD%A6%E4%B9%A0/NTU-ML-2022/ML2022-Lecture1-Notes/","content":"课程网站\nWhat is Machine Learning?\n\nMachine Learning ≈ Looking for Function\n\nDifferent types\n\nRegression(回归): 输出要预测的数值\nClassification(分类): 输出给定的options中的某一个\nStructured Learing: 创造有结构的物件(作图,写文章)\n\n从最基本的Linear Model开始\n例: 预测youtube频道第二天播放量\n\n构建模型\n基本术语:\n\nmodel: 带有未知参数的函数, 这里就是y=b+wx1y = b + wx_1y=b+wx1​\nfeature: x1x_1x1​, 已知的数据\nweight, bias: 分别是www和bbb\n\n定义损失函数\n一般可以表示为:\nL=1N∑nenL = \\frac{1}{N}\\sum_{n}e_n\nL=N1​n∑​en​\n其中e代表每组数据预测值跟真实值的误差, e可以有不同的计算方法, 如MAE/MSE等。\nError Surface\n测试不同的参数后画出的Loss的等高线图\n\n最优化(optimization)\n梯度下降(gradient descent)\n\n随机挑选初始值w0,b0w^0,b^0w0,b0\n更新参数:\n\nw1←w0−η∂L∂w∣w=w0,b=b0b1←b0−η∂L∂b∣w=w0,b=b0\\begin{align*} w^1 &amp;\\leftarrow w^0 - \\eta \\left. \\frac{\\partial L}{\\partial w} \\right|_{w=w^0, b=b^0} \\\\ b^1 &amp;\\leftarrow b^0 - \\eta \\left. \\frac{\\partial L}{\\partial b} \\right|_{w=w^0, b=b^0} \\end{align*}\nw1b1​←w0−η∂w∂L​​w=w0,b=b0​←b0−η∂b∂L​​w=w0,b=b0​​\n\n迭代\n\n其中η\\etaη就是学习率(learning rate), 一般要手动设定。\n我们把ML中需要手动设定的参数称为hyperparameter\n问题: 局部最优解(local minima) (?)\n\n并非真正的难题\n更加复杂的model\n关于激活函数\n\n大部分复杂的函数图像理论上都可以使用大量的蓝色折线图叠加来表达\n但是我们又该怎么表示这个「蓝色的折线图」呢?\n🤓☝️我们有sigmoid函数\ny=c11+e−(b+wx1)=c sigmoid(b+wx1)\\begin{align*} y &amp;= c \\frac{1}{1 + e^{-(b + wx_1)}} \\\\ &amp;= c\\,sigmoid(b + wx_1) \\end{align*}\ny​=c1+e−(b+wx1​)1​=csigmoid(b+wx1​)​\n而实际上这条「蓝色的折线图」就通常被称为「Hard Sigmoid」\n改变参数对图像的影响:\n\n除了sigmoid函数, 我们还可以用ReLU函数:\n\n那么对于这样的sigmoid函数或者ReLU函数或者其他的函数, 我们就将其称为激活函数。\n以下我们基于使用sigmoid函数的情况。\nNeural Network\n对于一组feature, 我们有:\ny=b+∑ici sigmoid(bi+wix1)y = b + \\sum_{i}c_i\\,sigmoid(b_i + w_ix_1)\ny=b+i∑​ci​sigmoid(bi​+wi​x1​)\n如果我们想要同时考虑 jjj 组feature:\ny=b+∑ici sigmoid(bi+∑jwijxj)y = b + \\sum_{i}c_i\\,sigmoid(b_i + \\sum_{j}w_{ij}x_{j})\ny=b+i∑​ci​sigmoid(bi​+j∑​wij​xj​)\n我们引入线性代数中向量与矩阵的概念来简便地表示y:\n\n然后我们把a1,a2,...a_1,a_2,...a1​,a2​,...继续作为新的xxx, 引入更多未知的参数来让函数更复杂:\n\n然后我们就有了Neural Network (:\n\n\n为什么不在一层上增加更多的节点让网络「变宽」而是要多做几层让网络「变深」?\n以后再讲(((\n(后续可能会补链接)\n\n\n为什么不能一直增加深度?\n容易带来过拟合的问题: 在训练集上表现变好但在未知数据上表现变差\n\n\n","categories":["学习","NTU-ML-2022"],"tags":["Notes","ML","AI"]},{"title":"建站记录","url":"/%E5%AD%A6%E4%B9%A0/%E5%BB%BA%E7%AB%99/%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95/","content":"参考:\nObsidian + Hexo + Github Pages建站\nHexo\nObsidian+Git完美维护Hexo博客\n支持渲染双向链接:\nHexo 博客适配 Obsidian 新语法\n图床配置\n腾讯云对象存储COS自建图床并配置Obsidian自动上传\nNext主题配置\nNext\n","categories":["学习","建站"],"tags":["Obsidian","Hexo"]}]